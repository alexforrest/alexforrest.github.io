<!DOCTYPE html>
<html lang="en">
<head>
        <title>Logistic Regression</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.4.0/css/font-awesome.min.css" />
        <link rel="stylesheet" href="./theme/css/main.css" />
</head>
<body>

    <div class="main-nav-container">

        <div class="pure-g">
            <div class="pure-u-1 pure-u-lg-2-3">
                <div class="main-nav">
                    <ul class="main-nav-list">
                        <li class="main-nav-item"><a href="./" class="pure-menu-link">Alex Harlan</a></li>

                        <li class="main-nav-item active"><a href="./category/blog.html" class="pure-menu-link">Blog</a></li>
                        <li class="main-nav-item"><a href="./category/projects.html" class="pure-menu-link">Projects</a></li>
                    </ul>
                </div>
             </div>

             <div class="pure-u-1 pure-u-lg-1-3"></div>
        </div>

    </div>


<div class="page-container">
    <div class="entry-content">
        <div class="post-meta pure-g">
            <div class="pure-u-3-4 meta-data">
                <a href="./category/blog.html" class="category">Blog</a><br />

                <a class="author" href="./author/alex-harlan.html">Alex Harlan</a>
                &mdash; <abbr title="2018-05-25T00:00:00-07:00">Fri 25 May 2018</abbr>
            </div>
        </div>
    </div>

    <div class="article-header-container">
        <div class="background-image-container">

            <div class="background-image-small">
                
                <div class="title-container">
                    <h1>Logistic Regression</h1>
                    <h4>An Intuition</h4>
                </div>
            </div>
        </div>
    </div>

    <div class="entry-content">
        <article class="h-entry">
<header>

<section data-field="body" class="e-content">
<section name="e450" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3775" id="3775" class="graf graf--h3 graf--leading graf--title"></h3><h4 name="a38d" id="a38d" class="graf graf--h4 graf-after--h3 graf--subtitle"></h4><p name="ab2b" id="ab2b" class="graf graf--p graf-after--h4">Where does logistic regression come from and why does it work for classification problems? In a naive approach if we wanted to predict two classes we could set one class to 0 and the other to 1 (it doesn’t matter which), then we could perform a linear regression and assign values less than .5 to the zero class and values at or above .5 to the ones class. Here we are treating the linear regression model as if it were providing us with some sort of probability when its not clear that it does. Worse our predicted values can be outside of the interval [0, 1] when a true probability couldn’t. </p><p name="7fdf" id="7fdf" class="graf graf--p graf-after--p">Logistic regression aims to address these issues. We want a function that will bound our results between zero and one and give us some properties that one might expect of a probability distribution. </p><p name="e668" id="e668" class="graf graf--p graf-after--p">It so happens that log(x/(1-x)) will help us achieve the result we are looking for. </p><p name="4b6e" id="4b6e" class="graf graf--p graf--empty graf-after--p"><br></p><figure name="775d" id="775d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 350px; max-height: 228px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4%;"></div><img class="graf-image" data-image-id="0*KErpPlX4tu4bzGHO.png" data-width="350" data-height="228" src="https://cdn-images-1.medium.com/max/800/0*KErpPlX4tu4bzGHO.png"></div></figure><p name="2bda" id="2bda" class="graf graf--p graf--empty graf-after--figure"><br></p><p name="e599" id="e599" class="graf graf--p graf-after--p">Instead of x we use p(x), where p(x) is a function that represents the conditionally probability of predicting the positive class given the negative class. We can make this function linear by doing the following.</p><figure name="f847" id="f847" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 344px; max-height: 102px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4%;"></div><img class="graf-image" data-image-id="1*-2sASTvHqgOC58obnxSr9Q.png" data-width="344" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*-2sASTvHqgOC58obnxSr9Q.png"></div></figure><p name="dcb4" id="dcb4" class="graf graf--p graf-after--figure">We then solve for p(x) by rewriting the equation in exponential form and doing some basic manipulation.</p><figure name="7905" id="7905" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 349px; max-height: 341px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4%;"></div><img class="graf-image" data-image-id="1*Oz1z7Ltv4o2EI9dCX9FcWw.png" data-width="349" data-height="341" src="https://cdn-images-1.medium.com/max/800/1*Oz1z7Ltv4o2EI9dCX9FcWw.png"></div></figure><p name="b120" id="b120" class="graf graf--p graf-after--figure">So p(x) can actually be described by what is known as the sigmoid function. </p><p name="cda3" id="cda3" class="graf graf--p graf--empty graf-after--p"><br></p><figure name="c69c" id="c69c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 402px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 1%;"></div><img class="graf-image" data-image-id="0*NWovILJGz9BBCVPm" data-width="602" data-height="402" src="https://cdn-images-1.medium.com/max/800/0*NWovILJGz9BBCVPm"></div><figcaption class="imageCaption"><a href="https://www.quora.com/Why-is-logistic-regression-considered-a-linear-model" data-href="https://www.quora.com/Why-is-logistic-regression-considered-a-linear-model" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank" text-align: center>source</a></figcaption></figure><p name="697f" id="697f" class="graf graf--p graf-after--figure">This function gives us the results we are looking for! Our output is on the interval (0, 1) and while this is not the same as [0, 1] we can get close enough to zero and one that it is functionally the same, plus this behavior is actually helpful. </p><p name="d4e2" id="d4e2" class="graf graf--p graf-after--p">How is it helpful? Well we said we wanted our function to behave as we might expect a distribution to behave. The asymptotes at zero and one give us the behavior that as our values become more extreme the change in probability becomes smaller. This is pretty informal but you might think of this via example. For instance, if you wash your hands each day you are less likely to get sick. However, increasing the number of times you wash your hands each day will result in diminishing returns to your probability of staying healthy after a certain number of washings. Someone who washes their hands 50 times a day is not significantly less likely to get sick than someone who washes 10 times. </p><p name="2d50" id="2d50" class="graf graf--p graf-after--p">This type of behavior is what we want from our probability distribution and presents a much better solution than using a linear regression classifier. Now we can assign predictions less than .5 to the negative class and at or above .5 to the positive class with the confidence that all our predictions will fall within the interval (0, 1).</p><p name="e874" id="e874" class="graf graf--p graf-after--p">This ends our basic introduction to the intuition behind logistic regression. We should keep in mind that so far we have only fitted our logistic regression to a simple linear regression, it is possible to extend this to multiple linear regression but the calculations are cleaner and easier to understand if we stick to simple linear regression. </p><p name="5242" id="5242" class="graf graf--p graf--empty graf-after--p"><br></p><p name="db0f" id="db0f" class="graf graf--p graf--empty graf-after--p"><br></p><p name="5a27" id="5a27" class="graf graf--p graf--empty graf-after--p"><br></p><p name="8638" id="8638" class="graf graf--p graf--empty graf-after--p graf--trailing"><br></p></div></div></section>
</section>
</article>
    </div>

    <footer>
        <div class="tags">
            <a href="./tag/logit.html">logit</a>
        </div>
        <div class="pure-g post-footer">
            <div class="pure-u-1 pure-u-md-1-2">
                <div class="pure-g poster-info">
                    <div class="pure-u-3-4">
                        <h3 class="author-name"><a href="./author/alex-harlan.html">Alex Harlan</a></h3>
                        <p class="author-description">
                        </p>
                    </div>
                </div>
            </div>



        </div>


    </footer>


</div>


</body>
</html>